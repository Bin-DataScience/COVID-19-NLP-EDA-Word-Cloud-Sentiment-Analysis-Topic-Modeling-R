---
title: "Covid_Tweets_R"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install.packages("BiocManager")
# BiocManager::install("Rgraphviz")
```


```{r, echo=FALSE}
# Load libraries
library(streamR)
library(tidyverse)
library(scales) 
library(tm) # Text Mining
library(SnowballC) # Snowball Stemmers
library(wordcloud)
library(wordcloud2) # wordcloud output in HTML format
library(RColorBrewer)
library(tidytext)
library(reshape2)
library(gridExtra)
library(corrplot)
library(ggmap)
library(igraph)
library(leaflet)
library(knitr)
library(Rgraphviz) #the developer version of Bioconductor package Rgraphviz
library(ggplot2)
library(fpc) # clustering
library(topicmodels)
library(data.table)

library(textdata)
```




```{r}
df_o <- read.csv("covidtweets.csv", stringsAsFactors = FALSE)
```

```{r}
head(df_o)
```

```{r}
str(df_o)
```

```{r}
dim(df_o)
```

```{r}
df <- sample_n(df_o, 5000, replace = FALSE)
```

```{r}
dim(df)
```

# Tweets per Minute

```{r}
df %>%
  mutate(created_at = substr(created_at, 12, 16)) %>% 
  count(created_at) %>% 
  ggplot(aes(x=as.numeric(as.factor(created_at)), y=n, group=1)) +
  geom_line(size=1, show.legend=FALSE) +
  labs(x='UCT Time', y='Number of Tweets') +
  theme_update() +
  scale_x_time()
  
```

# Geographic Information

```{r}
# how many locations are represented
length(unique(df$user_location))
```

```{r, fig.width=7, fig.height=100}
#df %>%
#  ggplot(aes(user_location)) +
#  geom_bar() + coord_flip() +
#     labs(x = "Count",
#      y = "Location",
#      title = "Twitter users - unique locations ")
```

```{r}
df %>%
  count(user_location, sort = TRUE) %>%
  mutate(location = reorder(user_location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Where Twitter users are from - unique locations ")
```


```{r}
# Delete rwos with blank values in user_location column
t <- df[!(is.na(df$user_location) | df$user_location==""), ]
```

```{r}
# view(t)
```

```{r}
t %>%
  count(user_location, sort = TRUE) %>%
  mutate(location = reorder(user_location,n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location,y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Location",
      y = "Count",
      title = "Twitter users - unique locations ")
```






```{r}
location_df = df['user_location']

```


# Most frequent languages

```{r}
df %>%
  count(lang) %>%
  arrange(desc(n)) %>%
  head(n=10) %>%
  ggplot(aes(x=reorder(lang, -n), y=n)) +
  geom_bar(stat="identity", fill="lightcyan", colour="black") +
  labs(x="Language", y="Frequency") + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 
  # scale_x_discrete(labels=c("English","Spanish", "Undefined", "German","Hindi", "Turkish","Tagalog","French","Indonesian","NA"))


```

# Characters and words

```{r}
# Histogram
df %>%
  filter(lang=="en" | lang=="es") %>%
  ggplot(aes(x=nchar(text), fill=lang)) +
  geom_histogram(bins=10, show.legend=FALSE) +
  facet_wrap(~lang) +
  theme_bw() +
  labs(x="Characters", y="Frequency") 
```

```{r}
# Density plot
df %>%
  filter(lang=="en" | lang=="es") %>%
  ggplot(aes(x=nchar(text), fill=lang)) +
  geom_density(alpha=0.5) +
  xlim(c(0, 150)) +
  theme_bw() +
  labs(x="Characters", y="Density") +
  guides(fill=guide_legend(title="Language"))
```

```{r}
# Boxplot
df %>%
  filter(lang=="en" | lang=="es") %>%
  ggplot(aes(x=lang, y=nchar(text), fill=lang)) +
  geom_boxplot(show.legend=FALSE) +
  ylim(c(0, 150)) +
  theme_bw() +
  labs(x="Language") +
  theme(axis.title.y=element_blank())
```

```{r}
# Histogram
df %>%
  mutate(words_per_tweet=sapply(strsplit(text, " "), length)) %>%
  filter(lang=="en" | lang=="es") %>%
  ggplot(aes(x=words_per_tweet, fill=lang)) +
  geom_histogram(bins=10, show.legend=FALSE) +
  xlim(c(0,40)) +
  theme_bw() +
  facet_wrap(~lang) +
  labs(x="Words", y="Frequency")
```

# User attributes
We are going to analize the following user attributes:

friends_count. The number of users this account is following.

followers_count. The number of followers this user currently has.

favourites_count. The number of Tweets this user has liked in the account’s lifetime.

statuses_count. The number of Tweets (including retweets) issued by the user.

```{r}
df %>%
  # User attributes
  select(user_friends_count, user_followers_count,
         user_favourites_count, user_statuses_count) %>%
  # Variables as values of a new column (facet_wrap)
  gather(Attribute, Num, 1:4) %>%
  mutate_at(vars(Attribute), factor) %>%
  ggplot(aes(x=Num, fill=Attribute)) +
  geom_histogram(bins=20, show.legend=FALSE) +
  xlim(c(0,2000)) +
  facet_wrap(~Attribute) +
  theme_bw() +
  labs(y="Frequency") +
  theme(axis.title.x=element_blank())
```

```{r}
# Correlation between number of followers and number of friends
ggplot(data=df, aes(x=user_followers_count, y=user_friends_count)) +
  geom_point(alpha=0.1) + 
  xlim(0, quantile(df$followers_count, 0.95, na.rm=TRUE)) +
  ylim(0, quantile(df$friends_count, 0.95, na.rm=TRUE)) + 
  geom_smooth(method="lm", color="red") +
  theme_bw() +
  labs(x="Number of followers", y="Number of friends") 
```

# WordCloud

```{r}
# view(df$user_description)
```

```{r}
#Create a vector containing only the text
description <- df$user_description
```

```{r}
# Create a corpus  
docs <- Corpus(VectorSource(description))
```

```{r}
# Clean the text data
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
description_df <- data.frame(word = names(words),freq=words)
```

```{r}
# Generate a classic wordcloud
set.seed(1234) # for reproducibility 
wordcloud(words = description_df$word, freq = description_df$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

```{r}
# Generate a wordcloud2
wordcloud2(description_df, minRotation=-pi/6, maxRotation=-pi/6, rotateRatio=1)
```





```{r}
# Build a corpus
tweet_corpus <- Corpus(VectorSource(df$text))
```

```{r}
#inspect a particular document
writeLines(as.character(tweet_corpus[[30]]))
```

# Corpus Transformations

```{r}
getTransformations()
```


```{r, echo=F}
#convert to lower case
tweet_corpus <- tm_map(tweet_corpus, content_transformer(tolower))
```

```{r}
# remove punctuation
tweet_corpus <- tm_map(tweet_corpus, removePunctuation) 
```

```{r}
# remove numbers
tweet_corpus <- tm_map(tweet_corpus, removeNumbers)
```
```{r}
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x) 
tweet_corpus <- tm_map(tweet_corpus, content_transformer(removeURL)) 
```


```{r}
# remove stopwords from corpus
tweet_corpus <- tm_map(tweet_corpus, removeWords, stopwords('english'))
```

```{r}
# keep a copy of corpus to use later as a dictionary for stem completion
tweet_corpus_Copy <- tweet_corpus
```

```{r}
# stem words
tweet_corpus  <- tm_map(tweet_corpus , stemDocument)
```

```{r}
# inspect the first 5 documents (tweets) 
inspect(tweet_corpus[1:5]) 
```

```{r}
# stem completion
# tweet_corpus <- tm_map(tweet_corpus, content_transformer(stemCompletion), dictionary = tweet_corpus_Copy)
```

```{r}
my_corpus <- tm_map(tweet_corpus, content_transformer(gsub), pattern = "harri", replacement = "harry")
```

```{r}
writeLines(as.character(my_corpus[[30]]))
```

```{r}
inspect(my_corpus[1:5]) 
```

# Document Term Matrix

```{r}
my_corpus2 <- VCorpus(VectorSource(df['text']))
```

```{r}
my_corpus2
```
```{r}
dtm1 <- DocumentTermMatrix(my_corpus2)
```

```{r}
inspect(dtm1)
```

```{r}
# coerces my_corpus into a Document Term Matrix
dtm_potter <- DocumentTermMatrix(my_corpus)
```

```{r}
# inspects chapters 1:5, terms 10:17
inspect(dtm_potter[1:5, 10:17]) 
```

```{r}
dtm_potter
```

# Word Frequency

```{r}
# Sum all columns(words) to get frequency
words_frequency <- colSums(as.matrix(dtm_potter)) 
```

```{r}
# verify that the terms are still equal to dtm_potter
length(words_frequency) 
```

```{r}
# create sort order (descending) for matrix
ord <- order(words_frequency, decreasing=TRUE)

# get the top 10 words by frequency of appeearance
words_frequency[head(ord, 10)] %>% 
  kable()
```

```{r}
#inspect least frequently occurring terms
words_frequency[tail(ord, 10)]
```

```{r}
wf=data.frame(term=names(words_frequency),occurrences=words_frequency)

p <- ggplot(subset(wf, words_frequency>400), aes(term, occurrences))
p <- p + geom_bar(stat='identity')
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
```

```{r}
p <- ggplot(subset(wf, words_frequency>200), aes(term, occurrences))
p <- p + geom_bar(stat='identity') + xlab("Terms") + ylab("Count") +coord_flip()
p
```


# count frequency of “mining”

```{r}
tdm <- TermDocumentMatrix(my_corpus, control = list(wordLengths = c(1, Inf)))
tdm
```

```{r}
# dimnames(tdm)$Terms
```



```{r}
## Freqency words and Association
idx <- which(dimnames(tdm)$Terms == "virus")
inspect(tdm[idx + (0:5), 101:110])
```


```{r}
#inspect frequent words
(freq.terms <- findFreqTerms(tdm, lowfreq=200))
```

```{r}
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >=200)
term.df <- data.frame(term = names(term.freq), freq = term.freq)
```

```{r}
ggplot(term.df, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
```


```{r}
plot(tdm, term = freq.terms, corThreshold = 0.12, weighting = T)
```

# TF:IDF

```{r}
(dtm_potter)
```

```{r}
# convert our "clean" corpus into a tfidf weighted dtm
DocumentTermMatrix(my_corpus, control = list(weighting = weightTfIdf)) -> dtm_potter_tfidf

# View details of tfidf weighted dtm
dtm_potter_tfidf
```

```{r}
# view details of chapters 1-17, terms 15:19
inspect(dtm_potter_tfidf) 
```

```{r}
# convert dtm into a df
df_potter <- tidy(dtm_potter)
```

```{r}
(bind_tf_idf)
```

```{r}
# take the product of tf and idf and create new column labeled "tf_idf". Graph it. 
bind_tf_idf(df_potter, term, document, count) %>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(term, levels = rev(unique(term))),
               chapter = factor(document)) %>%  
  group_by(document) %>% 
  top_n(6, wt = tf_idf) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = document)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        labs(title = "Highest tf-idf words in Philosopher's Stone by Chapter",
             x = "Words", y = "tf-idf") +
        facet_wrap(~chapter, ncol = 2, scales = "free") +
        coord_flip()
```

```{r}
bind_tf_idf(df_potter, term, document, count) %>% 
  arrange(desc(tf_idf)) 
  
```

# Word Cloud

```{r}
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min frequency
wordcloud(names(words_frequency),words_frequency, min.freq=200)
```


```{r}
#…add color
wordcloud(names(words_frequency),words_frequency,min.freq=150,colors=brewer.pal(6,'Dark2'))
```

```{r}
matrix <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(matrix), decreasing = T)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 200, random.order = F)
```

```{r}
# Generate a wordcloud2
#Create a vector containing only the text
text_words <- sort(rowSums(matrix),decreasing=TRUE) 
text_df <- data.frame(word = names(text_words),freq=text_words)

wordcloud2(data=text_df, size=1.6, color='random-dark')
```

```{r}
# Generate another wordcloud2
wordcloud2(data=text_df, size = 0.7, shape = 'pentagon')
```


```{r}
# Return unoredered frequent terms that appeared more than 200, but less than infinity.
findFreqTerms(dtm_potter, lowfreq = 200, highfreq = Inf)
```

# Clustering

```{r}
# remove sparse terms
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D")

plot(fit)
rect.hclust(fit, k = 6) # cut tree into 6 clusters 

```

```{r}
m3 <- t(m2) # transpose the matrix to cluster documents (tweets)
set.seed(122) # set a fixed random seed
k <- 6 # number of clusters
kmeansResult <- kmeans(m3, k)
round(kmeansResult$centers, digits = 3) # cluster centers

```

```{r}
# partitioning around medoids with estimation of number of clusters
pamResult <- pamk(m3, metric="manhattan")
```

```{r}
k <- pamResult$nc # number of clusters identified
```

```{r}
pamResult <- pamResult$pamobject
```

```{r}
# print cluster medoids
for (i in 1:k) {
    cat("cluster", i, ": ",
        colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n")
}

```

```{r}
# plot clustering result
layout(matrix(c(1, 2), 1, 2)) # set to two graphs per page
plot(pamResult, col.p = pamResult$clustering)
```


# Word Association

```{r}
# Find words that are correlated with "outbreak" with a coefficient > .20
findAssocs(dtm_potter, "outbreak", .20) %>% 
  kable()
```

```{r}
# find correlation coefficient of two words as a matrix with words as column headers
cor(as.matrix(dtm_potter)[, "virus"], as.matrix(dtm_potter)[, "wuhan"])
```


# Topic Model

```{r}
dtm <- as.DocumentTermMatrix(tdm)
```

```{r}
lda <- LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 4) # first 4 terms of every topic
term
```

```{r}
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
```


# Sentiment Analysis

## bing lexicon

```{r}
# Tokens
tokens <- df %>%  
  unnest_tokens(word, text) %>%
  select(word)
```

```{r}
# Positive and negative words 
tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=200)
```

## nrc lexicon

```{r}
# Sentiments and frequency associated with each word  
sentiments <- tokens %>% 
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort=TRUE) 


```

```{r}
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, n, sum), y=n)) + 
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw() +
coord_flip()
```

```{r}
# Top 10 frequent terms for each sentiment
sentiments %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Words") +
  coord_flip() 
```

## Sentiment analysis over time 


```{r}
df %>%  
  unnest_tokens(word, text) %>%
  select(word, created_at) %>%
  inner_join(get_sentiments("nrc")) %>%
  mutate(created_at=substr(created_at, 5, 7)) %>%
  count(created_at, sentiment) %>%
  ggplot(aes(x=as.numeric(as.factor(created_at)), y=as.factor(sentiment))) +
  geom_tile(aes(fill=n),  show.legend=FALSE) +
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(x="Month", y="Sentiment") +   
  #scale_fill_gradient(low="yellow", high="red") +
  scale_x_continuous(breaks=c(1,2,3,4),
                     labels=c("March", "April", "May", "June")) +
  labs(fill="Frequency")
```





